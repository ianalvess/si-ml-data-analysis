{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Notas para grupo:\n",
    "Este notebook precisa dos ficheiro \"train_dataset_reduced.csv\" e \"test_dataset_reduced.csv\" resultante da secção \"Feature Reduction\" da section1.ipynb!\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Supervised Learning\n",
    "# 1. Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.883186</td>\n",
       "      <td>-32.419895</td>\n",
       "      <td>-25.984499</td>\n",
       "      <td>-27.170945</td>\n",
       "      <td>-8.254103</td>\n",
       "      <td>-5.776573</td>\n",
       "      <td>0.982782</td>\n",
       "      <td>-2.485685</td>\n",
       "      <td>-3.561229</td>\n",
       "      <td>8.203845</td>\n",
       "      <td>...</td>\n",
       "      <td>11.874145</td>\n",
       "      <td>17.828786</td>\n",
       "      <td>0.265993</td>\n",
       "      <td>10.428243</td>\n",
       "      <td>2.542172</td>\n",
       "      <td>3.065818</td>\n",
       "      <td>-10.819577</td>\n",
       "      <td>18.087701</td>\n",
       "      <td>-5.237822</td>\n",
       "      <td>7.693530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.956624</td>\n",
       "      <td>-32.261849</td>\n",
       "      <td>-26.198370</td>\n",
       "      <td>-6.949561</td>\n",
       "      <td>-15.218213</td>\n",
       "      <td>-6.127422</td>\n",
       "      <td>-14.024204</td>\n",
       "      <td>-4.431317</td>\n",
       "      <td>-3.666016</td>\n",
       "      <td>-3.850515</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.242318</td>\n",
       "      <td>-1.009452</td>\n",
       "      <td>-5.394606</td>\n",
       "      <td>10.160249</td>\n",
       "      <td>2.913603</td>\n",
       "      <td>2.006827</td>\n",
       "      <td>-4.003897</td>\n",
       "      <td>20.003672</td>\n",
       "      <td>-5.326780</td>\n",
       "      <td>7.778053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-14.723855</td>\n",
       "      <td>-32.523669</td>\n",
       "      <td>-26.064893</td>\n",
       "      <td>-29.234224</td>\n",
       "      <td>-12.796521</td>\n",
       "      <td>-5.145179</td>\n",
       "      <td>14.705017</td>\n",
       "      <td>-0.323213</td>\n",
       "      <td>-3.637927</td>\n",
       "      <td>2.506266</td>\n",
       "      <td>...</td>\n",
       "      <td>2.515197</td>\n",
       "      <td>-6.600798</td>\n",
       "      <td>-6.517921</td>\n",
       "      <td>9.717316</td>\n",
       "      <td>2.732579</td>\n",
       "      <td>2.402578</td>\n",
       "      <td>-12.988763</td>\n",
       "      <td>18.686697</td>\n",
       "      <td>-5.099503</td>\n",
       "      <td>-1.198505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.747729</td>\n",
       "      <td>-31.692607</td>\n",
       "      <td>-26.488269</td>\n",
       "      <td>-1.667412</td>\n",
       "      <td>0.103331</td>\n",
       "      <td>-5.947562</td>\n",
       "      <td>-2.040008</td>\n",
       "      <td>-3.033317</td>\n",
       "      <td>-3.771963</td>\n",
       "      <td>0.204655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463400</td>\n",
       "      <td>1.131672</td>\n",
       "      <td>-4.616850</td>\n",
       "      <td>10.331812</td>\n",
       "      <td>2.695227</td>\n",
       "      <td>1.851255</td>\n",
       "      <td>-3.627302</td>\n",
       "      <td>19.746012</td>\n",
       "      <td>-5.287399</td>\n",
       "      <td>-5.139971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-15.628135</td>\n",
       "      <td>-32.152743</td>\n",
       "      <td>-26.273555</td>\n",
       "      <td>-0.103855</td>\n",
       "      <td>-21.175220</td>\n",
       "      <td>-6.354293</td>\n",
       "      <td>-16.859674</td>\n",
       "      <td>-4.189447</td>\n",
       "      <td>-4.530825</td>\n",
       "      <td>-13.664398</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.014334</td>\n",
       "      <td>-2.309537</td>\n",
       "      <td>-5.454498</td>\n",
       "      <td>10.194586</td>\n",
       "      <td>2.574430</td>\n",
       "      <td>1.711069</td>\n",
       "      <td>-3.284036</td>\n",
       "      <td>20.146148</td>\n",
       "      <td>-5.303387</td>\n",
       "      <td>4.175936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4         5  \\\n",
       "0  -18.883186 -32.419895 -25.984499 -27.170945  -8.254103 -5.776573   \n",
       "1  -17.956624 -32.261849 -26.198370  -6.949561 -15.218213 -6.127422   \n",
       "2  -14.723855 -32.523669 -26.064893 -29.234224 -12.796521 -5.145179   \n",
       "3  100.747729 -31.692607 -26.488269  -1.667412   0.103331 -5.947562   \n",
       "4  -15.628135 -32.152743 -26.273555  -0.103855 -21.175220 -6.354293   \n",
       "\n",
       "           6         7         8          9  ...         90         91  \\\n",
       "0   0.982782 -2.485685 -3.561229   8.203845  ...  11.874145  17.828786   \n",
       "1 -14.024204 -4.431317 -3.666016  -3.850515  ...  -1.242318  -1.009452   \n",
       "2  14.705017 -0.323213 -3.637927   2.506266  ...   2.515197  -6.600798   \n",
       "3  -2.040008 -3.033317 -3.771963   0.204655  ...  -0.463400   1.131672   \n",
       "4 -16.859674 -4.189447 -4.530825 -13.664398  ...  -2.014334  -2.309537   \n",
       "\n",
       "         92         93        94        95         96         97        98  \\\n",
       "0  0.265993  10.428243  2.542172  3.065818 -10.819577  18.087701 -5.237822   \n",
       "1 -5.394606  10.160249  2.913603  2.006827  -4.003897  20.003672 -5.326780   \n",
       "2 -6.517921   9.717316  2.732579  2.402578 -12.988763  18.686697 -5.099503   \n",
       "3 -4.616850  10.331812  2.695227  1.851255  -3.627302  19.746012 -5.287399   \n",
       "4 -5.454498  10.194586  2.574430  1.711069  -3.284036  20.146148 -5.303387   \n",
       "\n",
       "          Y  \n",
       "0  7.693530  \n",
       "1  7.778053  \n",
       "2 -1.198505  \n",
       "3 -5.139971  \n",
       "4  4.175936  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the pre-processed and reduced .csv file for the train dataset\n",
    "data_train = pd.read_csv(\"train_dataset_reduced.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.598474</td>\n",
       "      <td>-26.172290</td>\n",
       "      <td>-18.164093</td>\n",
       "      <td>16.565588</td>\n",
       "      <td>5.626666</td>\n",
       "      <td>-16.542026</td>\n",
       "      <td>-19.191404</td>\n",
       "      <td>13.729092</td>\n",
       "      <td>2.792426</td>\n",
       "      <td>-3.913262</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.097787</td>\n",
       "      <td>12.625538</td>\n",
       "      <td>-2.854209</td>\n",
       "      <td>1.547426</td>\n",
       "      <td>0.936617</td>\n",
       "      <td>2.255688</td>\n",
       "      <td>2.637114</td>\n",
       "      <td>-0.603131</td>\n",
       "      <td>1.228108</td>\n",
       "      <td>-66.134218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.142712</td>\n",
       "      <td>-16.259492</td>\n",
       "      <td>-16.443894</td>\n",
       "      <td>22.279652</td>\n",
       "      <td>2.313781</td>\n",
       "      <td>-6.178866</td>\n",
       "      <td>-8.116666</td>\n",
       "      <td>5.198965</td>\n",
       "      <td>13.231349</td>\n",
       "      <td>-17.754224</td>\n",
       "      <td>...</td>\n",
       "      <td>3.541037</td>\n",
       "      <td>1.072917</td>\n",
       "      <td>-0.474666</td>\n",
       "      <td>6.410620</td>\n",
       "      <td>3.696963</td>\n",
       "      <td>1.735389</td>\n",
       "      <td>-0.110205</td>\n",
       "      <td>1.028903</td>\n",
       "      <td>-0.237654</td>\n",
       "      <td>-3.809199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101.167952</td>\n",
       "      <td>14.222815</td>\n",
       "      <td>2.115523</td>\n",
       "      <td>-1.534398</td>\n",
       "      <td>4.589586</td>\n",
       "      <td>-2.012409</td>\n",
       "      <td>-6.125826</td>\n",
       "      <td>-15.230711</td>\n",
       "      <td>-2.442983</td>\n",
       "      <td>-6.382867</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848671</td>\n",
       "      <td>-0.439176</td>\n",
       "      <td>-1.728905</td>\n",
       "      <td>-0.170125</td>\n",
       "      <td>1.688246</td>\n",
       "      <td>-0.707996</td>\n",
       "      <td>-0.217847</td>\n",
       "      <td>0.162138</td>\n",
       "      <td>1.676366</td>\n",
       "      <td>14.652960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.080047</td>\n",
       "      <td>-21.132139</td>\n",
       "      <td>7.559850</td>\n",
       "      <td>25.816801</td>\n",
       "      <td>-2.693881</td>\n",
       "      <td>-13.839799</td>\n",
       "      <td>10.335748</td>\n",
       "      <td>-7.179723</td>\n",
       "      <td>5.868546</td>\n",
       "      <td>-21.504664</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.552970</td>\n",
       "      <td>-4.845901</td>\n",
       "      <td>-2.613450</td>\n",
       "      <td>-4.797476</td>\n",
       "      <td>-12.325194</td>\n",
       "      <td>-1.761066</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>6.280244</td>\n",
       "      <td>38.934764</td>\n",
       "      <td>-16.204517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.975281</td>\n",
       "      <td>-25.758489</td>\n",
       "      <td>26.517110</td>\n",
       "      <td>-4.107782</td>\n",
       "      <td>-15.526826</td>\n",
       "      <td>1.186478</td>\n",
       "      <td>-4.936762</td>\n",
       "      <td>-1.366573</td>\n",
       "      <td>7.566408</td>\n",
       "      <td>9.079443</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.166013</td>\n",
       "      <td>-2.169253</td>\n",
       "      <td>-1.025343</td>\n",
       "      <td>-4.884228</td>\n",
       "      <td>4.762200</td>\n",
       "      <td>-0.138442</td>\n",
       "      <td>3.945513</td>\n",
       "      <td>5.353743</td>\n",
       "      <td>37.779366</td>\n",
       "      <td>-17.080158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5  \\\n",
       "0  -18.598474 -26.172290 -18.164093  16.565588   5.626666 -16.542026   \n",
       "1  -17.142712 -16.259492 -16.443894  22.279652   2.313781  -6.178866   \n",
       "2  101.167952  14.222815   2.115523  -1.534398   4.589586  -2.012409   \n",
       "3  -10.080047 -21.132139   7.559850  25.816801  -2.693881 -13.839799   \n",
       "4   99.975281 -25.758489  26.517110  -4.107782 -15.526826   1.186478   \n",
       "\n",
       "           6          7          8          9  ...         90         91  \\\n",
       "0 -19.191404  13.729092   2.792426  -3.913262  ... -11.097787  12.625538   \n",
       "1  -8.116666   5.198965  13.231349 -17.754224  ...   3.541037   1.072917   \n",
       "2  -6.125826 -15.230711  -2.442983  -6.382867  ...   2.848671  -0.439176   \n",
       "3  10.335748  -7.179723   5.868546 -21.504664  ...  -5.552970  -4.845901   \n",
       "4  -4.936762  -1.366573   7.566408   9.079443  ...  -1.166013  -2.169253   \n",
       "\n",
       "         92        93         94        95        96        97         98  \\\n",
       "0 -2.854209  1.547426   0.936617  2.255688  2.637114 -0.603131   1.228108   \n",
       "1 -0.474666  6.410620   3.696963  1.735389 -0.110205  1.028903  -0.237654   \n",
       "2 -1.728905 -0.170125   1.688246 -0.707996 -0.217847  0.162138   1.676366   \n",
       "3 -2.613450 -4.797476 -12.325194 -1.761066  0.291961  6.280244  38.934764   \n",
       "4 -1.025343 -4.884228   4.762200 -0.138442  3.945513  5.353743  37.779366   \n",
       "\n",
       "           Y  \n",
       "0 -66.134218  \n",
       "1  -3.809199  \n",
       "2  14.652960  \n",
       "3 -16.204517  \n",
       "4 -17.080158  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the pre-processed and reduced .csv file for the test dataset\n",
    "data_test = pd.read_csv(\"test_dataset_reduced.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Train Dataset\n",
      "Train dataset shape: 18442 rows x 100 columns.\n",
      "Train dataset has NaNs?: False\n",
      "\n",
      "Descriptive Statistics of target column 'Y' for train dataset:\n",
      "count    18442.000000\n",
      "mean         5.117212\n",
      "std         22.902877\n",
      "min       -326.464304\n",
      "25%         -5.581255\n",
      "50%          4.337603\n",
      "75%         15.099934\n",
      "max        179.123325\n",
      "Name: Y, dtype: float64\n",
      "------------------------------\n",
      "Verifying Test Dataset\n",
      "Test dataset shape: 4610 rows x 100 columns.\n",
      "Test dataset has NaNs?: False\n",
      "\n",
      "Descriptive Statistics of target column 'Y' for test dataset:\n",
      "count    4610.000000\n",
      "mean        5.242951\n",
      "std        22.847305\n",
      "min      -242.808727\n",
      "25%        -5.386006\n",
      "50%         4.459068\n",
      "75%        15.004705\n",
      "max       169.716658\n",
      "Name: Y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verifying Train Dataset\n",
    "print(f\"Verifying Train Dataset\")\n",
    "print(f\"Train dataset shape: {data_train.shape[0]} rows x {data_train.shape[1]} columns.\")\n",
    "print(f\"Train dataset has NaNs?: {data_train.isnull().values.any()}\")\n",
    "print(f\"\\nDescriptive Statistics of target column 'Y' for train dataset:\\n{data_train['Y'].describe()}\")\n",
    "\n",
    "print('-' * 30)\n",
    "\n",
    "# Verifying Test Dataset\n",
    "print(f\"Verifying Test Dataset\")\n",
    "print(f\"Test dataset shape: {data_test.shape[0]} rows x {data_test.shape[1]} columns.\")\n",
    "print(f\"Test dataset has NaNs?: {data_test.isnull().values.any()}\")\n",
    "print(f\"\\nDescriptive Statistics of target column 'Y' for test dataset:\\n{data_test['Y'].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set \t Shape of X_train: (18442, 99) \n",
      "\t\t Shape of y_train: (18442,) \n",
      "\n",
      "Test Set \t Shape of X_test: (4610, 99) \n",
      "\t\t Shape of y_test: (4610,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting X and Y on the train and test datasets\n",
    "# For train dataset\n",
    "X_train = data_train.drop('Y', axis=1)  # Features\n",
    "y_train = data_train['Y']               # Target\n",
    "\n",
    "# For test dataset\n",
    "X_test = data_test.drop('Y', axis=1)    # Features\n",
    "y_test = data_test['Y']                 # Target\n",
    "\n",
    "# Checking the shape of the resulting sets\n",
    "print(f'Training Set \\t Shape of X_train: {X_train.shape} \\n\\t\\t Shape of y_train: {y_train.shape} \\n')\n",
    "print(f'Test Set \\t Shape of X_test: {X_test.shape} \\n\\t\\t Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train and Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Ensures features are scaled appropriately. Procedure won't be necessary if data was already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From this histogram, we can observe that the target variable for both Train and Test have similar distributions, indicating that the Test set is representative of the Train set.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verify the distribution of the target variable 'Y' on the Train and Test sets\n",
    "plt.hist(y_train, bins=50, alpha=0.5, label='Train set')\n",
    "plt.hist(y_test, bins=50, alpha=0.5, label='Test set')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Target variable for both Train and Test sets have similar distributions, indicating that the Test set is representative of the Train set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Multiple Models\n",
    "**We will train and test the following models:**\n",
    "- Ridge Regression (Linear + Regularization)\n",
    "- Random Forest Regressor (Ensemble)\n",
    "- KNeighbors Regressor (Instance-Based)\n",
    "- SVR (Kernel-Based)\n",
    "\n",
    "**Future Models to Try Out**\n",
    "- Gradient Boosting Algorithms (e.g., AdaBoost)\n",
    "\n",
    "\n",
    "**Metrics used to evaluate model:**\n",
    "\n",
    "- Mean Squared Error (MSE):\n",
    "    - Measures the average squared difference between predicted and actual values.\n",
    "\n",
    "- Mean Absolute Error (MAE):\n",
    "    - Measures the average absolute difference between predicted and actual values.\n",
    "\n",
    "- R-Squared (R²):\n",
    "    - Measures the proportion of variance in the target variable (`y_test`) explained by the model.\n",
    "    - A value closer to 1 indicates a better fit, while negative values suggest the model is worse than a simple mean prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing model: Ridge...\n",
      "Model: Ridge\n",
      "R2 Score: -0.1167\n",
      "MSE: 442.7453\n",
      "MAE: 15.6770\n",
      "------------------------------\n",
      "Training and Testing model: RandomForestRegressor...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining and Testing model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     29\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and Test multiple models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    Ridge(random_state=42),                                                  \n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),  # n_jobs=-1 ensure all CPU cores are used.\n",
    "    KNeighborsRegressor(),                              \n",
    "    SVR()\n",
    "]\n",
    "\n",
    "# Initialize metrics list of each model for comparison\n",
    "metrics_summary = []\n",
    "\n",
    "# Iterate over models and calculate metrics\n",
    "for model in models:\n",
    "    # Indicate the current model being trained\n",
    "    print(f\"Training and Testing model: {model.__class__.__name__}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Print metrics for the model\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"R2 Score: {test_r2:.4f}\")\n",
    "    print(f\"MSE: {test_mse:.4f}\")\n",
    "    print(f\"MAE: {test_mae:.4f}\")\n",
    "    print('-' * 30)\n",
    "\n",
    "    # Store metrics and model name\n",
    "    metrics_summary.append({\n",
    "        'model': model.__class__.__name__,\n",
    "        'r2_score': test_r2,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best R2 score: KNeighborsRegressor\n",
      "R2 Score: 0.2672\n",
      "\n",
      "Model with best MSE: KNeighborsRegressor\n",
      "MSE: 388.4052\n",
      "\n",
      "Model with best MAE: RandomForestRegressor\n",
      "MAE: 12.4902\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model based on R2 Score (higher is better)\n",
    "best_model_r2 = max(metrics_summary, key=lambda x: x['r2_score'])\n",
    "\n",
    "# Optionally, determine the best model based on MSE or MAE (lower is better)\n",
    "best_model_mse = min(metrics_summary, key=lambda x: x['mse'])\n",
    "best_model_mae = min(metrics_summary, key=lambda x: x['mae'])\n",
    "\n",
    "# Print the best models\n",
    "print(f\"Model with best R2 score: {best_model_r2['model']}\\nR2 Score: {best_model_r2['r2_score']:.4f}\\n\")\n",
    "print(f\"Model with best MSE: {best_model_mse['model']}\\nMSE: {best_model_mse['mse']:.4f}\\n\")\n",
    "print(f\"Model with best MAE: {best_model_mae['model']}\\nMAE: {best_model_mae['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ridge, R2 values: [0.24033767 0.20492267 0.22762115 0.22068466 0.19283378], Mean R2: 0.2172799849713118\n",
      "Model: Ridge, Negative MSE values: [-364.71165875 -424.87687566 -399.30378037 -383.05058035 -478.13589633], Mean MSE: 410.0157582910435\n",
      "Model: RandomForestRegressor, R2 values: [0.2956964  0.25366591 0.29054085 0.23968899 0.21006743], Mean R2: 0.25793191448837893\n",
      "Model: RandomForestRegressor, Negative MSE values: [-338.13409511 -398.82925093 -366.77560989 -373.7095313  -467.92731092], Mean MSE: 389.07515962776574\n",
      "Model: KNeighborsRegressor, R2 values: [0.25598179 0.27276844 0.28131966 0.24382136 0.24478589], Mean R2: 0.2597354290816519\n",
      "Model: KNeighborsRegressor, Negative MSE values: [-357.20096196 -388.62115697 -371.54276858 -371.67838495 -447.36135357], Mean MSE: 387.280925204932\n",
      "Model: SVR, R2 values: [0.23346734 0.20479118 0.22095106 0.22213775 0.18986043], Mean R2: 0.21424154998168604\n",
      "Model: SVR, Negative MSE values: [-368.01008503 -424.94714238 -402.75208075 -382.33635367 -479.89719997], Mean MSE: 411.58857235843453\n",
      "\n",
      "Summary of R2 scores: [0.2172799849713118, 0.25793191448837893, 0.2597354290816519, 0.21424154998168604]\n",
      "Summary of MSE: [410.0157582910435, 389.07515962776574, 387.280925204932, 411.58857235843453]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize lists to store results\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "# Iterate through each model\n",
    "for model in models:\n",
    "    # R2 scores\n",
    "    r2 = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    r2_scores.append(r2.mean())\n",
    "    print(f\"Model: {model.__class__.__name__}, R2 values: {r2}, Mean R2: {r2.mean()}\")\n",
    "\n",
    "    # MSE scores\n",
    "    mse = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_scores.append(-mse.mean())  # Negate to get positive MSE\n",
    "    print(f\"Model: {model.__class__.__name__}, Negative MSE values: {mse}, Mean MSE: {-mse.mean()}\\n\")\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\nSummary of R2 scores: {r2_scores}\")\n",
    "print(f\"Summary of MSE: {mse_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Results:\n",
      "\n",
      "MSE: 379.31\n",
      "MAE: 12.07\n",
      "R2: 0.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Models to include in the ensemble\n",
    "ensemble_model = VotingRegressor(estimators=[\n",
    "    (\"Ridge\", Ridge()),\n",
    "    (\"RandomForestRegression\", RandomForestRegressor()),\n",
    "    (\"KNeighborsRegressor\", KNeighborsRegressor()),\n",
    "    (\"SVR\", SVR())\n",
    "])\n",
    "\n",
    "# Training the ensemble\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform the predictions using the ensemble\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble performance and print the results\n",
    "print(f\"Ensemble Results:\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_ensemble):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_ensemble):.2f}\")\n",
    "print(f\"R2: {r2_score(y_test, y_pred_ensemble):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "MSE: 359.75\n",
      "MAE: 11.73\n",
      "R2 Score: 0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100], \n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4]\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_test, y_test)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_rf = best_model.predict(X_test)\n",
    "\n",
    "# Print the Best Parameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Print Best Metrics\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred_rf):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
